import streamlit as st
import torch
import time
import os
import requests
from PIL import Image, ImageEnhance, ImageFilter
import pytesseract
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    MBartForConditionalGeneration,
    MBart50TokenizerFast,
)
from transformers import AutoProcessor, AutoModelForCausalLM
from google.cloud import texttospeech
import io
import re  # OSDì—ì„œ íšŒì „ ê°ë„ ì¶”ì¶œìš©

# ===================================================================
# 0. ì„¤ì • ë° ì¸ì¦
# ===================================================================

# Tesseract OCR ê²½ë¡œ (ì‹¤ì œ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

# Papago API ì¸ì¦ ì •ë³´ (ì‹¤ì œ ë°°í¬ ì‹œì—ëŠ” í™˜ê²½ë³€ìˆ˜ ì‚¬ìš© ê¶Œì¥)
# -> ê¹ƒí—ˆë¸Œ ì˜¬ë¦´ ë•ŒëŠ” ê¼­ ì´ ê°’ë“¤ ì§€ìš°ê³  í™˜ê²½ë³€ìˆ˜ë¡œ ë°”ê¿”ë¼.
PAPAGO_CLIENT_ID = "4vhevyvhqf"
PAPAGO_CLIENT_SECRET = "RrVqhvpZyjcIj1dhjRqJ47T7DTBCaniCV0gn0J3M"

# Google Cloud ì¸ì¦ ê²½ë¡œ (ì‹¤ì œ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
GOOGLE_CREDENTIALS_PATH = r"C:\Users\user\PycharmProjects\PythonProject5\tenacious-post-332905-7cd866ce3088.json"
if os.path.exists(GOOGLE_CREDENTIALS_PATH):
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = GOOGLE_CREDENTIALS_PATH
else:
    st.error("Google API ì¸ì¦ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")

# ì–¸ì–´ ì½”ë“œ ë§¤í•‘
PAPAGO_LANG_MAP = {"í•œêµ­ì–´": "ko", "ì˜ì–´": "en", "ì¼ë³¸ì–´": "ja"}
MBART_LANG_MAP = {"í•œêµ­ì–´": "ko_KR", "ì˜ì–´": "en_XX", "ì¼ë³¸ì–´": "ja_XX"}
GOOGLE_TTS_STT_MAP = {
    "í•œêµ­ì–´": "ko-KR",
    "ì˜ì–´": "en-US",
    "ì¼ë³¸ì–´": "ja-JP",
}  # TTSìš©

# Tesseractìš© ì–¸ì–´ ë§¤í•‘ (ì‹¬í™” OCRì—ì„œ ì‚¬ìš©)
TESSERACT_LANG_MAP = {
    "í•œêµ­ì–´": "kor",
    "ì˜ì–´": "eng",
    "ì¼ë³¸ì–´": "jpn",
}


# ===================================================================
# 1. ëª¨ë¸ ë¡œë“œ ë° ìºì‹± í•¨ìˆ˜
# ===================================================================


@st.cache_resource
def load_captioning_model():
    """GIT Image Captioning ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ìºì‹±í•©ë‹ˆë‹¤."""
    try:
        MODEL_NAME = "microsoft/git-base"
        caption_processor = AutoProcessor.from_pretrained(MODEL_NAME)
        caption_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)
        st.success("âœ… GIT ìº¡ì…”ë‹ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ì˜¤í”„ë¼ì¸)")
        return {"processor": caption_processor, "model": caption_model}, None
    except Exception as e:
        st.error(f"âŒ GIT ìº¡ì…”ë‹ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, f"GIT ìº¡ì…”ë‹ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}"


@st.cache_resource
def load_mbart_model():
    """mBART ë¡œì»¬ ë²ˆì—­ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ìºì‹±í•©ë‹ˆë‹¤."""
    try:
        MBART_MODEL_NAME = "facebook/mbart-large-50-many-to-many-mmt"
        tokenizer = MBart50TokenizerFast.from_pretrained(MBART_MODEL_NAME)
        model = MBartForConditionalGeneration.from_pretrained(MBART_MODEL_NAME)
        st.success("âœ… mBART ë²ˆì—­ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ì˜¤í”„ë¼ì¸)")
        return {"tokenizer": tokenizer, "model": model}, None
    except Exception as e:
        st.error(f"âŒ mBART ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, f"mBART ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}"


# ===================================================================
# 2. OCR ì „ì²˜ë¦¬ ë° ì‹¤í–‰ í•¨ìˆ˜
# ===================================================================


def preprocess_for_ocr(image: Image.Image) -> Image.Image:
    """
    ì‹¬í™” OCRìš© ì „ì²˜ë¦¬:
    - ê·¸ë ˆì´ìŠ¤ì¼€ì¼(í‘ë°±) ë³€í™˜
    - íšŒì „(ê¸°ìš¸ê¸°) ë³´ì • (OSD)
    - ì‘ì€ ì´ë¯¸ì§€ í•´ìƒë„ ì—…ìƒ˜í”Œë§
    - ë…¸ì´ì¦ˆ ì œê±° (MedianFilter)
    - ëŒ€ë¹„ ê°•í™” + ì´ì§„í™”
    """
    # 1. ê·¸ë ˆì´ìŠ¤ì¼€ì¼(í‘ë°±)
    img = image.convert("L")

    # 2. OSDë¥¼ ì´ìš©í•œ íšŒì „ ê°ë„ ì¶”ì • í›„ deskew
    try:
        osd = pytesseract.image_to_osd(img)
        angle_search = re.search(r"Rotate: (\d+)", osd)
        if angle_search:
            angle = float(angle_search.group(1))
            if angle != 0:
                # ìˆ˜í‰ì„ ë§ì¶”ë„ë¡ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ íšŒì „
                img = img.rotate(-angle, expand=True)
    except Exception:
        # OSD ì‹¤íŒ¨ ì‹œì—ëŠ” íšŒì „ ë³´ì • ìƒëµ
        pass

    # 3. í•´ìƒë„ ì—…ìƒ˜í”Œë§ (í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì‘ì„ ë•Œ)
    w, h = img.size
    max_side = max(w, h)
    if max_side < 1000:
        scale = 1000 / max_side
        new_size = (int(w * scale), int(h * scale))
        img = img.resize(new_size, Image.LANCZOS)

    # 4. ë…¸ì´ì¦ˆ ì œê±° (MedianFilter)
    img = img.filter(ImageFilter.MedianFilter(size=3))

    # 5. ëŒ€ë¹„ ê°•í™”
    enhancer = ImageEnhance.Contrast(img)
    img = enhancer.enhance(2.0)

    # 6. ì´ì§„í™” (ì™„ì „ í‘/ë°±)
    threshold = 128
    img = img.point(lambda x: 0 if x < threshold else 255, "1")

    return img


def run_ocr(image: Image.Image, lang_code: str, psm: int = 6) -> str:
    """
    ê³µí†µ OCR ì‹¤í–‰ í•¨ìˆ˜
    - lang_code: 'kor', 'eng', 'jpn', 'kor+eng+jpn' ë“±
    - psm: page segmentation mode (6: ë‹¨ë½, 11: í•œ ì¤„ ë“±)
    """
    custom_config = f"--oem 3 --psm {psm}"
    text = pytesseract.image_to_string(image, lang=lang_code, config=custom_config)
    return text.strip()


def is_valid_ocr_text(text: str) -> bool:
    """
    OCR ê²°ê³¼ê°€ 'ì½ì„ ë§Œí•œ ë¬¸ì¥'ì¸ì§€ ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ íŒë³„.
    ë„ˆë¬´ ì§§ê±°ë‚˜, íŠ¹ìˆ˜ë¬¸ì ë¹„ìœ¨ì´ ë„ˆë¬´ ë†’ìœ¼ë©´ ë…¸ì´ì¦ˆë¡œ ê°„ì£¼.
    """
    if not text:
        return False

    t = text.strip()
    if len(t) < 15:  # 15ì ë¯¸ë§Œì´ë©´ ì—†ëŠ” ê±¸ë¡œ
        return False

    t_no_nl = t.replace("\n", " ")
    letters = sum(ch.isalpha() for ch in t_no_nl)
    digits = sum(ch.isdigit() for ch in t_no_nl)
    symbols = sum((not ch.isalnum() and not ch.isspace()) for ch in t_no_nl)
    total = len(t_no_nl)

    if letters + digits < 5:
        return False

    if total > 0 and symbols / total > 0.35:
        return False

    return True


# ===================================================================
# 3. ê¸°ëŠ¥ ì‹¤í–‰ í•¨ìˆ˜ (ìº¡ì…”ë‹ / ë²ˆì—­ / TTS)
# ===================================================================


def generate_image_caption(image, caption_tools):
    """ì´ë¯¸ì§€ ê°ì²´ë¥¼ ì…ë ¥ë°›ì•„ ìº¡ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤ (GIT ëª¨ë¸)."""
    start_time = time.time()
    try:
        if not caption_tools:
            return "ìº¡ì…”ë‹ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨", 0

        processor = caption_tools["processor"]
        model = caption_tools["model"]

        image_rgb = image.convert("RGB")

        inputs = processor(images=image_rgb, return_tensors="pt")

        # pixel_values ì°¨ì› ê°•ì œ ì •ê·œí™”
        pixel_values = inputs.pixel_values
        if pixel_values.dim() == 3:
            pixel_values = pixel_values.unsqueeze(0)  # [1, 3, H, W]
        elif pixel_values.shape[0] != 1:
            pixel_values = pixel_values[0].unsqueeze(0)  # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë§Œ ì‚¬ìš©

        generated_ids = model.generate(
            pixel_values=pixel_values, max_length=50, num_beams=5
        )
        caption = processor.batch_decode(
            generated_ids, skip_special_tokens=True
        )[0]

        end_time = time.time()
        return caption, (end_time - start_time) * 1000

    except Exception as e:
        return f"ìº¡ì…˜ ìƒì„± ì˜¤ë¥˜: {type(e).__name__} - {e}", 0


def translate_papago(text, target_lang, source_lang):
    """Papago APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë²ˆì—­í•©ë‹ˆë‹¤ (ì‚¬ìš©ì ì§€ì • ì›ë³¸ ì–¸ì–´)."""
    start_time = time.time()
    target_code = PAPAGO_LANG_MAP.get(target_lang, "en")
    source_code = PAPAGO_LANG_MAP.get(source_lang, "en")

    url = "https://papago.apigw.ntruss.com/nmt/v1/translation"
    headers = {
        "X-NCP-APIGW-API-KEY-ID": PAPAGO_CLIENT_ID,
        "X-NCP-APIGW-API-KEY": PAPAGO_CLIENT_SECRET,
        "Content-Type": "application/x-www-form-urlencoded",
    }
    data = {"source": source_code, "target": target_code, "text": text}
    translated_text = text

    try:
        response = requests.post(url, headers=headers, data=data)
        response.raise_for_status()
        result = response.json()["message"]["result"]
        translated_text = result.get("translatedText", text)
        src_lang_returned = result.get("srcLangType", source_code)

        normalized_text = "".join(text.lower().split())
        normalized_translated = "".join(translated_text.lower().split())

        if normalized_translated == normalized_text and src_lang_returned != target_code:
            st.caption(
                f"âš ï¸ Papago ë²ˆì—­ ê²°ê³¼ê°€ ì›ë¬¸ê³¼ ë™ì¼í•©ë‹ˆë‹¤. (ì›ë¬¸: {source_lang}, ëª©í‘œ: {target_lang})"
            )

        end_time = time.time()
        return translated_text, (end_time - start_time) * 1000

    except Exception as e:
        return (
            f"Papago ì˜¤ë¥˜: {e} - ì‘ë‹µ: {response.text if 'response' in locals() else 'ì—†ìŒ'}",
            0,
        )


def translate_mbart(text, target_lang, mbart_tools, source_lang):
    """mBART ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë²ˆì—­í•©ë‹ˆë‹¤ (ì˜¤í”„ë¼ì¸, ì¼â†’í•œ ì˜ì–´ ìš°íšŒ ë¡œì§ í¬í•¨)."""
    start_time = time.time()

    m_tools = mbart_tools
    if not m_tools:
        return "mBART ë¡œë“œ ì‹¤íŒ¨", 0

    tokenizer = m_tools["tokenizer"]
    model = m_tools["model"]

    try:
        if source_lang == target_lang:
            return text, 0.0

        if source_lang == "ì¼ë³¸ì–´" and target_lang == "í•œêµ­ì–´":
            # 1ë‹¨ê³„: ì¼ë³¸ì–´ -> ì˜ì–´
            tokenizer.src_lang = MBART_LANG_MAP["ì¼ë³¸ì–´"]
            encoded_ja_to_en = tokenizer(text, return_tensors="pt")
            generated_en = model.generate(
                **encoded_ja_to_en,
                forced_bos_token_id=tokenizer.lang_code_to_id[MBART_LANG_MAP["ì˜ì–´"]],
            )
            english_text = tokenizer.decode(
                generated_en[0], skip_special_tokens=True
            )

            # 2ë‹¨ê³„: ì˜ì–´ -> í•œêµ­ì–´
            tokenizer.src_lang = MBART_LANG_MAP["ì˜ì–´"]
            encoded_en_to_ko = tokenizer(english_text, return_tensors="pt")
            generated_ko = model.generate(
                **encoded_en_to_ko,
                forced_bos_token_id=tokenizer.lang_code_to_id[MBART_LANG_MAP["í•œêµ­ì–´"]],
            )
            translated_text = tokenizer.decode(
                generated_ko[0], skip_special_tokens=True
            )

        else:
            src_code_mbart = MBART_LANG_MAP[source_lang]
            tgt_code_mbart = MBART_LANG_MAP[target_lang]

            tokenizer.src_lang = src_code_mbart
            encoded = tokenizer(text, return_tensors="pt")

            generated = model.generate(
                **encoded,
                forced_bos_token_id=tokenizer.lang_code_to_id[tgt_code_mbart],
            )
            translated_text = tokenizer.decode(
                generated[0], skip_special_tokens=True
            )

        end_time = time.time()
        return translated_text, (end_time - start_time) * 1000

    except Exception as e:
        return f"mBART ë²ˆì—­ ì˜¤ë¥˜: {e}", 0


@st.cache_resource
def get_tts_client():
    """Google TextToSpeechClient í´ë¼ì´ì–¸íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    return texttospeech.TextToSpeechClient()


def synthesize_google_cloud_tts(text, lang_code):
    """Google Cloud TTS APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    start_time = time.time()
    tts_client = get_tts_client()
    synthesis_input = texttospeech.SynthesisInput(text=text)

    voice_name_map = {
        "ko-KR": "ko-KR-Wavenet-D",
        "en-US": "en-US-Wavenet-D",
        "ja-JP": "ja-JP-Wavenet-D",
    }

    voice = texttospeech.VoiceSelectionParams(
        language_code=lang_code,
        name=voice_name_map.get(lang_code, "en-US-Wavenet-D"),
    )
    audio_config = texttospeech.AudioConfig(
        audio_encoding=texttospeech.AudioEncoding.MP3
    )

    try:
        response = tts_client.synthesize_speech(
            input=synthesis_input,
            voice=voice,
            audio_config=audio_config,
        )
        end_time = time.time()
        return response.audio_content, (end_time - start_time) * 1000, None
    except Exception as e:
        return None, 0, f"Google Cloud TTS ì˜¤ë¥˜: {e}"


# ===================================================================
# 4. Streamlit UI ë©”ì¸ êµ¬ì„±
# ===================================================================


def multimodal_tts_app():
    st.set_page_config(layout="wide")
    st.title("ğŸ“¸ ì´ë¯¸ì§€ ìº¡ì…”ë‹ ë° ë‹¤êµ­ì–´ TTS")
    st.subheader("ì‹œê° ì¥ì• ì¸ ì ‘ê·¼ì„± ê°œì„ ì„ ìœ„í•œ ë©€í‹°ëª¨ë‹¬ AI íŒŒì´í”„ë¼ì¸ ì‹œë®¬ë ˆì´ì…˜")

    # 1. ëª¨ë¸ ë¡œë“œ
    caption_tools, caption_error = load_captioning_model()
    mbart_tools, mbart_error = load_mbart_model()

    if caption_error or mbart_error:
        st.warning("ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ë¡œ ì¼ë¶€ ê¸°ëŠ¥(ìº¡ì…”ë‹/ì˜¤í”„ë¼ì¸ ë²ˆì—­)ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    st.markdown("---")

    # 2. UI ìš”ì†Œ
    uploaded_image = st.file_uploader(
        "ì´ë¯¸ì§€ íŒŒì¼ ì—…ë¡œë“œ", type=["png", "jpg", "jpeg"], key="image_multimodal"
    )

    col_mode, col_lang, col_ocr_src = st.columns(3)

    with col_mode:
        translation_mode = st.radio(
            "â‘  OCR í…ìŠ¤íŠ¸ ë²ˆì—­ ì—”ì§„ ì„ íƒ",
            ["ì˜¨ë¼ì¸ (Papago API)", "ì˜¤í”„ë¼ì¸ (mBART ëª¨ë¸)"],
            key="trans_mode",
        )

    with col_lang:
        target_tts_lang = st.selectbox(
            "â‘¡ ìµœì¢… TTS ì¶œë ¥ ì–¸ì–´",
            ["í•œêµ­ì–´", "ì˜ì–´", "ì¼ë³¸ì–´"],
            key="tts_output_lang",
        )

    with col_ocr_src:
        ocr_source_lang = st.selectbox(
            "â‘¢ OCR í…ìŠ¤íŠ¸ ì›ë³¸ ì–¸ì–´",
            ["ì˜ì–´", "ì¼ë³¸ì–´", "í•œêµ­ì–´"],
            key="ocr_source_lang",
            help="ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì˜ ì‹¤ì œ ì–¸ì–´ë¥¼ ì„ íƒí•˜ì„¸ìš”.",
        )

    st.caption("ìº¡ì…”ë‹ì€ í•­ìƒ GIT (ì˜¤í”„ë¼ì¸)ë¡œ, TTSëŠ” Google Cloud TTS (ì˜¨ë¼ì¸)ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

    if st.button("â‘£ ì´ë¯¸ì§€ ë¶„ì„ ë° TTS ì‹¤í–‰", key="run_multimodal_pipeline"):
        if uploaded_image is None:
            st.warning("ì´ë¯¸ì§€ íŒŒì¼ì„ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
            return

        st.markdown("---")
        image = Image.open(uploaded_image)
        st.image(image, caption="ì›ë³¸ ì´ë¯¸ì§€", use_column_width=True)

        # 1ï¸âƒ£ ì´ë¯¸ì§€ ìº¡ì…”ë‹ (GIT ëª¨ë¸)
        with st.spinner("1/4: ì´ë¯¸ì§€ ë‚´ìš© ë¬˜ì‚¬ (ìº¡ì…”ë‹) ìƒì„± ì¤‘..."):
            caption_text, cap_time = generate_image_caption(image, caption_tools)
        if "ì˜¤ë¥˜" in caption_text:
            st.error(f"âŒ ìº¡ì…”ë‹ ì‹¤íŒ¨: {caption_text}")
            return
        st.success(
            f"âœ… ì´ë¯¸ì§€ ë¬˜ì‚¬ (ì›ë¬¸) [ì˜¤í”„ë¼ì¸, {cap_time:.2f}ms]: **{caption_text}**"
        )

        # 1-1ï¸âƒ£ ìº¡ì…˜ì„ ìµœì¢… ì–¸ì–´ë¡œ ë²ˆì—­ (GIT ìº¡ì…˜ ì›ë¬¸ì€ ì˜ì–´)
        with st.spinner("1-1/4: ìº¡ì…˜ì„ ìµœì¢… ì–¸ì–´ë¡œ ë²ˆì—­ ì¤‘..."):
            if translation_mode == "ì˜¨ë¼ì¸ (Papago API)":
                translated_caption, cap_trans_time = translate_papago(
                    caption_text, target_tts_lang, "ì˜ì–´"
                )
            else:
                translated_caption, cap_trans_time = translate_mbart(
                    caption_text, target_tts_lang, mbart_tools, "ì˜ì–´"
                )
        st.success(
            f"âœ… ìº¡ì…˜ ë²ˆì—­ [{target_tts_lang}, {cap_trans_time:.2f}ms]: **{translated_caption}**"
        )

        # 2ï¸âƒ£ OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ (1ì°¨: ì‚¬ìš©ìê°€ ê³ ë¥¸ ì–¸ì–´, 2ì°¨: í‘ë°±Â·ì‹¬í™” ì „ì²˜ë¦¬)
        with st.spinner("2/4: OCR (ì´ë¯¸ì§€ ë‚´ í…ìŠ¤íŠ¸) ì¶”ì¶œ ì¤‘..."):
            try:
                # ì‚¬ìš©ìê°€ ê³ ë¥¸ ì›ë³¸ ì–¸ì–´ ê¸°ì¤€ìœ¼ë¡œ Tesseract ì–¸ì–´ ê²°ì •
                tess_lang_main = TESSERACT_LANG_MAP.get(ocr_source_lang, None)

                # 1ì°¨: ì›ë³¸ ì´ë¯¸ì§€ + ë‹¨ì¼ ì–¸ì–´
                # ë§Œì•½ ë§¤í•‘ì„ ëª» ì°¾ìœ¼ë©´ ë§ˆì§€ë§‰ ì•ˆì „ì¥ì¹˜ë¡œ kor+eng+jpn
                first_pass_lang = (
                    tess_lang_main if tess_lang_main is not None else "kor+eng+jpn"
                )
                text_from_image = run_ocr(image, lang_code=first_pass_lang, psm=6)

                def shorten_text(t: str) -> str:
                    return t if len(t) <= 250 else t[:250] + "..."

                text_from_image = shorten_text(text_from_image)

                if text_from_image:
                    st.info(
                        f"âœ… OCR ì¶”ì¶œ í…ìŠ¤íŠ¸ (1ì°¨, {ocr_source_lang}): **{text_from_image}**"
                    )
                else:
                    st.warning("âš ï¸ 1ì°¨ OCRì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

                # 1ì°¨ ê²°ê³¼ê°€ ì—†ê±°ë‚˜, ë„ˆë¬´ ì§§ìœ¼ë©´ â†’ í‘ë°±Â·ì‹¬í™” ì „ì²˜ë¦¬ë¡œ ë™ì¼ ì–¸ì–´ë¡œ ì¬ì‹œë„
                if (not text_from_image) or (len(text_from_image) < 10):
                    st.caption(
                        "ğŸ” 1ì°¨ ì¸ì‹ì´ ì•½í•´, í‘ë°±Â·ì‹¬í™” ì „ì²˜ë¦¬ë¡œ ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤..."
                    )
                    bw_img = preprocess_for_ocr(image.copy())

                    # ì‹¬í™” ëª¨ë“œì—ì„œë„ ê°™ì€ ì–¸ì–´ ì‚¬ìš©
                    deep_lang = first_pass_lang
                    deep_text = run_ocr(bw_img, lang_code=deep_lang, psm=6)
                    deep_text = shorten_text(deep_text)

                    if deep_text:
                        st.info(
                            f"âœ… OCR ì¶”ì¶œ í…ìŠ¤íŠ¸ (2ì°¨, í‘ë°±Â·ì‹¬í™” / {ocr_source_lang}): **{deep_text}**"
                        )

                        # 2ì°¨ ê²°ê³¼ê°€ ë” ê¸¸ê±°ë‚˜, 1ì°¨ê°€ ì•„ì˜ˆ ì—†ìœ¼ë©´ 2ì°¨ ê²°ê³¼ë¥¼ ì±„íƒ
                        if (not text_from_image) or (
                            len(deep_text) > len(text_from_image)
                        ):
                            text_from_image = deep_text
                            st.success(
                                "â¡ ìµœì¢… OCR í…ìŠ¤íŠ¸ë¡œ 2ì°¨(í‘ë°±Â·ì‹¬í™”) ê²°ê³¼ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
                            )
                    else:
                        st.warning(
                            "âš ï¸ í‘ë°±Â·ì‹¬í™” ì „ì²˜ë¦¬ í›„ì—ë„ í…ìŠ¤íŠ¸ë¥¼ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                        )

            except Exception as e:
                st.error(f"âŒ OCR ì‹¤íŒ¨: {e}")
                text_from_image = ""

            # --- OCR ë…¸ì´ì¦ˆ í•„í„°ë§: ì´ìƒí•œ ë¬¸ìì—´ì´ë©´ ì•„ì˜ˆ ë²„ë¦°ë‹¤ ---
            if text_from_image:
                if not is_valid_ocr_text(text_from_image):
                    st.info(
                        "â„¹ï¸ ì´ë¯¸ì§€ ë‚´ í…ìŠ¤íŠ¸ê°€ ê±°ì˜ ì—†ê±°ë‚˜ ë…¸ì´ì¦ˆë¡œ íŒë‹¨ë˜ì–´, "
                        "OCR ê²°ê³¼ëŠ” TTSì— í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                    )
                    text_from_image = ""

        # 3ï¸âƒ£ OCR í…ìŠ¤íŠ¸ ë²ˆì—­
        translated_ocr_text = ""
        source_lang_for_translation = st.session_state.ocr_source_lang

        if text_from_image:
            if source_lang_for_translation == target_tts_lang:
                translated_ocr_text = text_from_image
                st.success(
                    f"âœ… ë²ˆì—­ ê²°ê³¼ [{target_tts_lang}, 0.00ms]: "
                    f"**ì›ë¬¸ê³¼ ëª©í‘œ ì–¸ì–´ê°€ ê°™ì•„ ë²ˆì—­ ìƒëµ**"
                )
            else:
                st.caption(
                    f"ë²ˆì—­ ì—”ì§„: {translation_mode}, ì›ë³¸ ì–¸ì–´: {source_lang_for_translation}"
                )
                with st.spinner("3/4: OCR í…ìŠ¤íŠ¸ ë²ˆì—­ ì¤‘..."):
                    if translation_mode == "ì˜¨ë¼ì¸ (Papago API)":
                        translated_ocr_text, trans_time = translate_papago(
                            text_from_image, target_tts_lang, source_lang_for_translation
                        )
                    else:
                        translated_ocr_text, trans_time = translate_mbart(
                            text_from_image,
                            target_tts_lang,
                            mbart_tools,
                            source_lang_for_translation,
                        )

                    if "ì˜¤ë¥˜" in translated_ocr_text:
                        st.error(f"âŒ ë²ˆì—­ ì‹¤íŒ¨: {translated_ocr_text}")
                    else:
                        st.success(
                            f"âœ… ë²ˆì—­ ê²°ê³¼ [{target_tts_lang}, {trans_time:.2f}ms]: "
                            f"**{translated_ocr_text}**"
                        )

        # 4ï¸âƒ£ ì–¸ì–´ë³„ ì•ˆë‚´ ë¬¸ì¥ êµ¬ì„±
        announce_texts = {
            "í•œêµ­ì–´": {
                "caption": "ì´ë¯¸ì§€ ë‚´ìš©ì…ë‹ˆë‹¤:",
                "ocr": "ê·¸ë¦¬ê³  ì´ë¯¸ì§€ì—ì„œ ë‹¤ìŒ í…ìŠ¤íŠ¸ê°€ ë²ˆì—­ë˜ì—ˆìŠµë‹ˆë‹¤:",
                "no_text": "ì´ë¯¸ì§€ ë‚´ í…ìŠ¤íŠ¸ëŠ” ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
            },
            "ì˜ì–´": {
                "caption": "The image shows:",
                "ocr": "And the text detected in the image has been translated as follows:",
                "no_text": "No text was found in the image.",
            },
            "ì¼ë³¸ì–´": {
                "caption": "ç”»åƒã®å†…å®¹ã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ï¼š",
                "ocr": "ãã—ã¦ç”»åƒå†…ã®ãƒ†ã‚­ã‚¹ãƒˆã¯æ¬¡ã®ã‚ˆã†ã«ç¿»è¨³ã•ã‚Œã¾ã—ãŸï¼š",
                "no_text": "ç”»åƒå†…ã®ãƒ†ã‚­ã‚¹ãƒˆã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚",
            },
        }

        phrases = announce_texts.get(target_tts_lang, announce_texts["í•œêµ­ì–´"])

        # 5ï¸âƒ£ ìµœì¢… TTS ë¬¸ì¥ êµ¬ì„±
        if translated_ocr_text and translated_ocr_text.strip():
            final_text_to_speak = (
                f"{phrases['caption']} {translated_caption}. "
                f"{phrases['ocr']} {translated_ocr_text}"
            )
        else:
            # í…ìŠ¤íŠ¸ê°€ ì—†ì„ ë•ŒëŠ” ì•ˆë‚´ ë¬¸ì¥ ì—†ì´ ìº¡ì…˜ë§Œ ì½ì–´ì¤Œ
            final_text_to_speak = f"{phrases['caption']} {translated_caption}"

        st.markdown("### ğŸ’¬ ìµœì¢… TTS í…ìŠ¤íŠ¸ (ë²ˆì—­ë¬¸ ë°˜ì˜)")
        st.success(final_text_to_speak)

        # 6ï¸âƒ£ Google Cloud TTS ì‹¤í–‰
        with st.spinner("4/4: ìµœì¢… í…ìŠ¤íŠ¸ë¥¼ Google Cloud TTSë¡œ í•©ì„± ì¤‘ (ê³ í’ˆì§ˆ)..."):
            google_tts_lang_code = GOOGLE_TTS_STT_MAP.get(target_tts_lang)
            google_audio, google_time, google_error = synthesize_google_cloud_tts(
                final_text_to_speak, google_tts_lang_code
            )

        st.subheader(f"ğŸ—£ï¸ TTS ê²°ê³¼ ({target_tts_lang}, Google Cloud TTS)")
        st.metric("í•©ì„± ì†Œìš” ì‹œê°„ (ms)", f"{google_time:.2f} ms")
        if google_error:
            st.error(f"âŒ í•©ì„± ì‹¤íŒ¨: {google_error}")
            st.warning(
                "Google Cloud TTS APIê°€ í™œì„±í™”ë˜ì—ˆëŠ”ì§€, ì¸ì¦ íŒŒì¼ ê²½ë¡œê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì„¸ìš”."
            )
        else:
            st.audio(google_audio, format="audio/mp3")
            st.success("âœ¨ ë‹¤êµ­ì–´ TTS ì™„ë£Œ!")


if __name__ == "__main__":
    multimodal_tts_app()
